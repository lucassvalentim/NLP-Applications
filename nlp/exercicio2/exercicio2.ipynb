{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importações necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "import re\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = pd.read_csv(\"data/texts.txt\", header=None, names=[\"message\"], sep=\"\\r\")\n",
    "label = pd.read_csv(\"data/score.txt\", header=None, names=[\"labels\"], sep=\"\\r\")\n",
    "\n",
    "topic_df = pd.concat([message, label], axis=1)\n",
    "# topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0 palavras/doc\n"
     ]
    }
   ],
   "source": [
    "# Tamanho medio dos documentos antes do pré-processamento\n",
    "sample_message = topic_df[\"message\"].tolist()\n",
    "num_words = [len(s.split()) for s in sample_message]\n",
    "print(f\"{np.median(num_words)} palavras/doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/valentim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') # download para fazer a lematização do texto usando WordNetLematizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contraction_text(texts):\n",
    "    ''' \n",
    "        This function is used to expand the possible hiring of some words in the text\n",
    "    '''\n",
    "    \n",
    "    expanded_text = []\n",
    "    for word in texts.split():\n",
    "        expanded_text.append(contractions.fix(word))\n",
    "\n",
    "    texts = ' '.join(expanded_text)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "def lemmatizer_text(texts):\n",
    "    ''' \n",
    "    function responsible for lemmatizing the text\n",
    "    '''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    word_token = texts.split()\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos='v') for word in word_token]\n",
    "\n",
    "    texts = ' '.join(lemmas)\n",
    "\n",
    "    return texts\n",
    "\n",
    "def processing(texts):\n",
    "    texts = contraction_text(texts)\n",
    "    texts = lemmatizer_text(texts)\n",
    "    \n",
    "    result = []\n",
    "    pos_tag = ['ADJ', 'NOUN', 'VERB', 'PROPN'] # vetor que permite escolher somente entre adjetivos, substantivos, verbos e pronomes\n",
    "    text = re.sub(r'(https?://[^\\s\\n\\r]+|www\\.[^\\s\\n\\r]+|[0-9@#&!?:,.\\)\\(;])', '', texts) # expressão regular que retira do texto todos os números, simbolos (&#;) e URL\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    for token in doc:\n",
    "        if(token.text in stop_words or token.text in punctuation or len(token.text) <= 3 or token.text == \"ltgt\"):\n",
    "            continue\n",
    "        \n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "        \n",
    "        text = ' '.join(str(element) for element in result if not element.isdigit())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>labels</th>\n",
       "      <th>processed_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How did serfdom develop in and then leave Russ...</td>\n",
       "      <td>0</td>\n",
       "      <td>serfdom develop leave russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "      <td>1</td>\n",
       "      <td>film feature character popeye doyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I find a list of celebrities ' real na...</td>\n",
       "      <td>0</td>\n",
       "      <td>find list celebrities real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What fowl grabs the spotlight after the Chines...</td>\n",
       "      <td>1</td>\n",
       "      <td>fowl grab spotlight chinese year monkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "      <td>2</td>\n",
       "      <td>form</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  labels  \\\n",
       "0  How did serfdom develop in and then leave Russ...       0   \n",
       "1   What films featured the character Popeye Doyle ?       1   \n",
       "2  How can I find a list of celebrities ' real na...       0   \n",
       "3  What fowl grabs the spotlight after the Chines...       1   \n",
       "4                    What is the full form of .com ?       2   \n",
       "\n",
       "                         processed_message  \n",
       "0             serfdom develop leave russia  \n",
       "1      film feature character popeye doyle  \n",
       "2               find list celebrities real  \n",
       "3  fowl grab spotlight chinese year monkey  \n",
       "4                                     form  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df.message = topic_df.message.astype(str)\n",
    "topic_df[\"processed_message\"] = topic_df[\"message\"].apply(processing)\n",
    "\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 palavras/doc\n"
     ]
    }
   ],
   "source": [
    "# Tamanho medio dos documentos antes do pré-processamento\n",
    "sample_message = topic_df[\"processed_message\"].tolist()\n",
    "num_words = [len(s.split()) for s in sample_message]\n",
    "print(f\"{np.median(num_words)} palavras/doc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF aplicada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(topic_df[\"message\"])\n",
    "words = np.array(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=20, solver=\"mu\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1 meaning,united,states,is,population,word,what,origin,of,the\n",
      "Topic: 2 old,died,were,long,much,have,people,there,many,how\n",
      "Topic: 3 prime,minister,killed,created,won,president,wrote,the,invented,who\n",
      "Topic: 4 long,word,take,an,have,it,stand,what,mean,does\n",
      "Topic: 5 if,make,find,get,why,say,call,how,you,do\n",
      "Topic: 6 new,used,movie,what,film,america,which,the,city,in\n",
      "Topic: 7 state,day,made,known,color,capital,called,an,what,is\n",
      "Topic: 8 about,the,located,information,get,come,from,find,can,where\n",
      "Topic: 9 american,by,woman,space,year,born,president,what,first,was\n",
      "Topic: 10 why,names,colors,cities,countries,some,two,there,what,are\n",
      "Topic: 11 another,famous,named,known,term,the,abbreviation,what,stand,for\n",
      "Topic: 12 way,used,the,first,long,from,take,be,it,to\n",
      "Topic: 13 children,cold,what,glass,water,being,food,kind,of,fear\n",
      "Topic: 14 begin,war,originate,become,come,play,what,die,year,did\n",
      "Topic: 15 its,with,by,real,dog,lawyer,craft,randy,that,name\n",
      "Topic: 16 wrote,as,children,by,two,with,what,difference,between,and\n",
      "Topic: 17 which,that,find,moon,continent,information,earth,internet,can,on\n",
      "Topic: 18 population,mountain,ii,highest,country,war,city,the,largest,world\n",
      "Topic: 19 been,which,popular,the,common,what,country,state,has,most\n",
      "Topic: 20 beethoven,the,died,he,nixon,begin,invented,day,born,when\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(H):\n",
    "    print(f\"Topic: {i + 1} {\",\".join(str(x) for x in words[topic.argsort()[-10:]])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
